# ğŸ“ Transformer-based Text Summarization

This repository contains my implementation of a **Transformer Summarizer** using an **Encoder-Decoder architecture**.  
The project demonstrates **how Transformers use attention mechanisms** to generate concise summaries of long texts.

This implementation builds a summarization model from scratch using **multi-head attention**, **positional encoding**, **causal masking**, and a **fully custom Transformer decoder**.

---

## ğŸš€ Project Overview

This project covers:

1. Implementing the **scaled dot-product attention** mechanism.
2. Building a **Transformer Decoder** from scratch.
3. Using **masking techniques** to manage dependencies.
4. Training the model for **abstractive summarization**.
5. Evaluating and generating **summaries for unseen text**.

---

## ğŸ“Œ Key Features
- ğŸ§  Implements the **Transformer architecture** from scratch.
- ğŸ§© Builds **custom encoder-decoder blocks**.
- ğŸ”¹ Uses **multi-head attention** and **positional encoding**.
- ğŸ› ï¸ Implements **causal masking** for sequential decoding.
- ğŸ“„ Generates **abstractive summaries** of long-form text.
- ğŸ–¼ï¸ Includes visualizations of **attention weights** and **training metrics**.

---

## ğŸ§© Project Workflow

| Step                        | Description |
|---------------------------|------------|
| **Data Preprocessing**    | Tokenizes and cleans text for summarization |
| **Positional Encoding**   | Encodes token positions for sequential awareness |
| **Scaled Dot-Product Attention** | Computes context-based attention scores |
| **Multi-Head Attention**  | Uses multiple heads for better context learning |
| **Decoder Implementation**| Builds the Transformer decoder block manually |
| **Training**              | Trains the summarizer on the dataset |
| **Evaluation**            | Generates summaries and evaluates performance |

---

## ğŸ“‚ Project Structure

```bash
transformer-text-summarizer/
â”‚â”€â”€ data/                  # Training & test datasets
â”‚â”€â”€ notebooks/             # Jupyter notebook implementation
â”‚â”€â”€ models/                # Saved models & checkpoints
â”‚â”€â”€ utils/                 # Helper functions for attention, masking, etc.
â”‚â”€â”€ results/               # Evaluation metrics & sample summaries
â”‚â”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md             # Project documentation
